# train a miniature character-level shakespeare model
# good for debugging and playing on macbooks and such
model: 
    # baby GPT model 
    n_layer: 6
    n_head: 6
    n_embd: 384
    dropout: 0.2
    block_size: 64 # context of up to 64 previous characters
    bias: false
dataset: 'additions_3_3'
training:
    init_from: 'scratch'
    eval_only: false
    out_dir: 'adder_1'
    eval_interval: 250 # keep frequent because we'll overfit
    eval_iters: 200
    log_interval: 10000 # don't print too too often

    # we expect to overfit on this small dataset, so only save when val improves
    always_save_checkpoint: false

    wandb_log: false # override via command line if you like
    wandb_project: 'addition_v1'
    wandb_run_name: 'mini-gpt'


    gradient_accumulation_steps: 1
    batch_size: 64
 

    learning_rate: 1e-3 # with baby networks can afford to go a bit higher
    max_iters: 5000
    weight_decay: 1e-1
    decay_lr: true # whether to decay the learning rate
    warmup_iters: 2000 # how many steps to warm up for
    lr_decay_iters: 5000 # make equal to max_iters usually
    min_lr: 1e-4 # learning_rate / 10 usually
    beta1: 0.9
    beta2: 0.99 # make a bit bigger because number of tokens per iter is small
    grad_clip: 1.0 # clip gradients at this value, or disable if == 0.0
    # learning rate decay settings

environment:
    device: 'cuda'
    # on macbook also add
    # device: 'cpu'  # run on cpu only
    # compile: false # do not torch compile the model
